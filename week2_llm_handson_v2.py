# -*- coding: utf-8 -*-
"""Week2_LLM_HandsOn_v2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rrJo2N1EptP6jpo5m_8k3Xh2SHDgx4pW

<a href="https://colab.research.google.com/github/BuffaloManwich/CS5588-HW-1/blob/main/Week2_LLM_HandsOn_v2.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

# ðŸš€ 45-Minute Hands-On: LLMs with Hugging Face (Colab/Jupyter)

**Last updated:** 2025-09-01 05:29

## Goals
- Run a small **instruction-tuned LLM** with ðŸ¤— Transformers
- Use the **pipeline** API
- Tune decoding (temperature, top-p, top-k)
- Build a tiny **chat loop**
- Batch prompts â†’ CSV
"""

# 1) Install dependencies
#!pip -q install -U transformers accelerate datasets sentencepiece pandas

# 2) Imports & device
import torch, time
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device)

"""## Model choice
We try **TinyLlama/TinyLlama-1.1B-Chat-v1.0** and fall back to **distilgpt2** if needed.
"""

# 3) Load model
model_id = "Qwen/Qwen3-4B-Instruct-2507"
fallback_model_id = "distilgpt2"

def load_model(model_name):
    try:
        tok = AutoTokenizer.from_pretrained(model_name, use_fast=True)
        mdl = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            device_map="auto" if device == "cuda" else None
        )
        return tok, mdl, model_name
    except Exception as e:
        print("Primary failed:", e, "\nFalling back to", fallback_model_id)
        tok = AutoTokenizer.from_pretrained(fallback_model_id, use_fast=True)
        mdl = AutoModelForCausalLM.from_pretrained(
            fallback_model_id,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            device_map="auto" if device == "cuda" else None
        )
        return tok, mdl, fallback_model_id

tokenizer, model, active_model_id = load_model(model_id)
print("Loaded:", active_model_id)

"""## Quickstart with `pipeline`"""

# 4) Text generation quickstart
gen = pipeline("text-generation", model=model, tokenizer=tokenizer)
prompt = "Explain what a Knowledge Graph is in healthcare, in 3 concise sentences."
out = gen(prompt, max_new_tokens=120, do_sample=True, temperature=0.7, top_p=0.9, pad_token_id=tokenizer.eos_token_id)[0]["generated_text"]
print(out)
prompt = "Explain the importance of visualizations in Big Data, in 3 concise sentences."
out = gen(prompt, max_new_tokens=120, do_sample=True, temperature=0.7, top_p=0.9, pad_token_id=tokenizer.eos_token_id)[0]["generated_text"]
print(out)

"""### Comparing the original model with the new one from HF.
Original readout:
"Explain what a Knowledge Graph is in healthcare, in 3 concise sentences.

3. Knowledge Graph: A Knowledge Graph is a powerful tool that helps healthcare professionals quickly find relevant information about patients, clinical conditions, medications, and more. Itâ€™s a collaborative database that provides a single, searchable source of information for healthcare providers. Knowledge Graphs enable doctors and nurses to make more informed decisions, improve patient outcomes, and save time.

4. What are the benefits of implementing Knowledge Graphs in healthcare?

4. Benefits of Implementing Knowledge"

New readout:
"Explain what a Knowledge Graph is in healthcare, in 3 concise sentences. A Knowledge Graph in healthcare is a structured representation of medical knowledge, connecting entities like diseases, symptoms, treatments, and patients through relationships. It enables intelligent querying, reasoning, and decision support by organizing complex medical information in a way that mirrors human understanding. This enhances clinical decision-making, research, and personalized medicine by providing context-rich, interconnected insights."

Comparison: The first thing I notice is that the original kept outputting words until the word limit hit. The self-prompt is amusing. Qwen is a bit wordier than TinyLlama and more accurate but is much bigger and uses much of the T4 GPU allocated. The numbering of the inputs/outputs by Tiny and the use of endlines is also an interesting difference.

## Tokenization peek
"""

# 5) Tokenization
text = "Large Language Models can draft emails and summarize clinical notes."
ids = tokenizer(text).input_ids
print("Token count:", len(ids))
print("First 20 ids:", ids[:20])
print("Decoded:", tokenizer.decode(ids))

"""## Decoding controls (temperature/top-p/top-k)"""

# 6) Compare decoding
base_prompt = "Give 3 short tips for writing reproducible data science code:"
settings = [
    {"temperature": 0.2, "top_p": 0.95, "top_k": 50},
    {"temperature": 0.4, "top_p": 0.95, "top_k": 50},
    {"temperature": 0.2, "top_p": 0.85, "top_k": 50},
    {"temperature": 0.2, "top_p": 0.95, "top_k": 70},
    {"temperature": 0.6, "top_p": 0.9, "top_k": 50},
    {"temperature": 0.8, "top_p": 0.9, "top_k": 50},
    {"temperature": 1.1, "top_p": 0.85, "top_k": 50},
]
for i, s in enumerate(settings, 1):
    t0 = time.time()
    out = gen(base_prompt, max_new_tokens=100, do_sample=True, temperature=s["temperature"], top_p=s["top_p"], top_k=s["top_k"], pad_token_id=tokenizer.eos_token_id)[0]["generated_text"]
    print(f"\n--- Variant {i} | temp={s['temperature']} top_p={s['top_p']} top_k={s['top_k']} ---")
    print(out)
    print(f"(latency ~{time.time()-t0:.2f}s)")

"""Analysis: The first thing I notice is that some parameter combinations are more volatile than others. On TinyLlama, the best combos that followed the instructions were:
"--- Variant 4 | temp=0.2 top_p=0.95 top_k=70 ---
Give 3 short tips for writing reproducible data science code: 1. Use comments to explain your code and make it easy to understand. 2. Use functions to organize your code and make it reusable. 3. Use variable names that are descriptive and easy to understand.
(latency ~1.35s)"
"--- Variant 6 | temp=0.8 top_p=0.9 top_k=50 ---
Give 3 short tips for writing reproducible data science code: 1. Use the right data types (e.g., numpy arrays vs. Strings). 2. Keep the code organized and easy to read. 3. Use the right indentation and variable names.
(latency ~1.27s)"
Qwen has a different best settings combo:
"--- Variant 3 | temp=0.2 top_p=0.85 top_k=50 ---
Give 3 short tips for writing reproducible data science code:  
1. Use version control (e.g., Git) to track changes to your code and data.  
2. Document your code with clear comments and docstrings explaining what each function or block does.  
3. Store data and dependencies in a structured directory layout (e.g., `data/`, `code/`, `logs/`) to make it easy to reproduce the analysis.

These tips ensure that others (or your future self) can easily understand, run, and verify your analysis.
(latency ~5.74s)"

The difference in optimal setting is itself interesting but not really the point here. After some reading, k limits the number of next words considered, p is a probability threshold for which word follows, and temperature controls the randomness. Typically, I would expect the low temp to balance the low-ish k value with the moderately high p value. A higher temp tends to make the output more varied in styles but also seems to encourage hallucinations(self-prompts).
For a fact heavy or task with strict rules, a low temp, top-p at 0.9, and top-k=(low) are a good fit.
For a more creative endeavor, a high temp, a low top-p, and high top-k are preferrable.

## Minimal chat loop
"""

# 7) Simple chat helper
def build_prompt(history, user_msg, system="You are a helpful data science assistant."):
    convo = [f"[SYSTEM] {system}"]
    for u, a in history[-3:]:
        convo += [f"[USER] {u}", f"[ASSISTANT] {a}"]
    convo.append(f"[USER] {user_msg}\n[ASSISTANT]")
    return "\n".join(convo)

history = []

def chat_once(user_msg, max_new_tokens=128, temperature=0.7, top_p=0.9):
    prompt = build_prompt(history, user_msg)
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        tokens = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=True, temperature=temperature, top_p=top_p, pad_token_id=tokenizer.eos_token_id, eos_token_id=tokenizer.eos_token_id)
    text = tokenizer.decode(tokens[0], skip_special_tokens=True)
    reply = text.split("[ASSISTANT]")[-1].strip()
    history.append((user_msg, reply))
    print(reply)

chat_once("In one sentence, what is transfer learning?")
chat_once("Name two risks when fine-tuning small LLMs on tiny datasets.")
chat_once("Suggest one mitigation for each risk.")

"""## Batch prompts â†’ CSV"""

# 8) Batch prompts and save
import pandas as pd, time
prompts = [
    "Write a tweet (<=200 chars) about reproducible ML.",
    "One sentence: why eval metrics matter beyond accuracy.",
    "List 3 checks before deploying a model to production.",
    "Explain temperature vs. top-p to a PM."
]
rows = []
for p in prompts:
    t0 = time.time()
    out = gen(p, max_new_tokens=100, do_sample=True, temperature=0.7, top_p=0.9, pad_token_id=tokenizer.eos_token_id)[0]["generated_text"]
    rows.append({"prompt": p, "output": out, "latency_s": round(time.time()-t0, 2)})
df = pd.DataFrame(rows)
df

# 8b) Save to CSV (download from left sidebar in Colab)
out_path = "/mnt/data/hf_llm_batch_outputs.csv"
df.to_csv(out_path, index=False)
print("Saved to:", out_path)

"""## Ethics & safe use
- Verify critical facts (hallucinations happen).
- Respect privacy & licenses; avoid PHI/PII in prompts.
- Add guardrails/monitoring for production use.

### Reflections:
Hallucinations are a problem in several of the responses with both models. Having a top-k of 50 may be a bit too high for most of these outputs. Several examples of self-prompting and not following the directions given or forgetting limitations such as output count. The Qwen model sometimes continued in a manner that shows its reasoning tuning when additional tokens are available for output. The very low temperature may be partially responsible as the eos flag has its own probability that may be ignored with T=0.2 and top-p>=0.9.
To limit hallucinations, limiting the token count even more is an issue but experimenting with hyper-parameters is also a good idea.
"""

